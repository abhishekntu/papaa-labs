{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution on Multi-core CPUs\n",
    "\n",
    "When targeting CPUs, the set of possible optimizations and opportunities for parallelization are limited. You can exploit multiple cores and split your computations across those. You can exploit SIMD units in each core to further boost performance. If your CPU has an embedded GPU (for example, the Intel Iris graphics cores), it is sometimes possible to use the GPU through the OpenCL environment. These tightly-integrated GPUs share the memory space, and do not require explicit copying that is otherwise necessary for accelerators. A side benefit of using OpenCL here is the future possibility of porting to another, newer, OpenCL-compatible platform. \n",
    "\n",
    "1. SIMD Vectorization:\n",
    "While most OpenCL compilers will auto-vectorize simple code, it is often necessary to explicitly use OpenCL vector types for ensuring high performance. This eliminates writing low-level intrinsics, and transparently allows the same OpenCL code to run on different hardware with varying SIMD vector widths. The **float4** type is an example of the OpenCL vector type. Apart from changing the type of your operation, you have to rearrnage your inputs to fit the vector access pattern. A 2D convolution rewritten for SIMD vectorization looks like one below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__kernel void convolve(\n",
    "        const __global float *in,               // W*H input images\n",
    "        __constant float *filt,                 // K*K filter kernel\n",
    "        __global float *out,                    // W*H output images\n",
    "        const int K,                            // filter resolution\n",
    "        const float pBias)                      // constant offset/bias\n",
    "{\n",
    "        // get pixel position\n",
    "        const int W = get_global_size(0);\n",
    "        const int H = get_global_size(1);\n",
    "\n",
    "        // get image resolution\n",
    "        const int x = get_global_id(0); \n",
    "        const int y = get_global_id(1);\n",
    "\n",
    "        float4 sum = 0;\n",
    "\n",
    "        // loop over rows\n",
    "        for (int r = 0; r < K; r++) \n",
    "        {\n",
    "                // loop over columns\n",
    "                for(int c = 0,c4 = 0; c < K, c4<ceil(K/4); c+=4,c4++)\n",
    "                {\n",
    "                        float4 filt4 = vload4(c4,filt[r*K]);\n",
    "                        float4 in4 = vload4(c4,in[(y+r)*W+x]);\n",
    "                        sum += filt4*in4;\n",
    "                }\n",
    "                // TODO: for the odd last element..\n",
    "        }\n",
    "        out[y*W+x] = sum.x + sum.y + sum.z + sum.w + pBias;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above works well for large values of K>5. For smaller values of K<=5, we may want to avoid vectorization.\n",
    "\n",
    "2. Loop Unrolling:\n",
    "We can also use loop unrolling as a way to improve performance. Loops, without extra information, are sequential operations. If the programmer (or compiler) can reason about data independence across loop iterations, we can run each loop iteration in parallel. A programmer can provide hints to the compiler about what loop to unroll and to what extent. We first show a simple manually unrolled OpenCL kernel, and its equivalent version with compiler hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__kernel void convolve(\n",
    "        const __global float *in,               // W*H input images\n",
    "        __constant float *filt,                 // K*K filter kernel\n",
    "        __global float *out,                    // W*H output images\n",
    "        const int K,                            // filter resolution\n",
    "        const float pBias)                      // constant offset/bias\n",
    "{\n",
    "        // get pixel position\n",
    "        const int W = get_global_size(0);\n",
    "        const int H = get_global_size(1);\n",
    "\n",
    "        // get image resolution\n",
    "        const int x = get_global_id(0); \n",
    "        const int y = get_global_id(1);\n",
    "\n",
    "        float sum = 0;\n",
    "        int c = 0;\n",
    "\n",
    "        // loop over rows\n",
    "        for (int r = 0, r < K, r++)\n",
    "        {\n",
    "                // loop over columns\n",
    "                for(c = 0, c < K, c+=2)\n",
    "                {\n",
    "                        sum += filt[r*K+c]*in[((y+r)*W+x)+c];\n",
    "                        sum += filt[r*K+c+1]*in[((y+r)*W+x)+c+1];\n",
    "                }\n",
    "        }\n",
    "        out[y*W+x] = sum + pBias;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__kernel void convolve(\n",
    "        const __global float *in,               // W*H input images\n",
    "        __constant float *filt,                 // K*K filter kernel\n",
    "        __global float *out,                    // W*H output images\n",
    "        const int K,                            // filter resolution\n",
    "        const float pBias)                      // constant offset/bias\n",
    "{\n",
    "        // get pixel position\n",
    "        const int W = get_global_size(0);\n",
    "        const int H = get_global_size(1);\n",
    "\n",
    "        // get image resolution\n",
    "        const int x = get_global_id(0); \n",
    "        const int y = get_global_id(1);\n",
    "\n",
    "        float sum = 0;\n",
    "        int c = 0;\n",
    "\n",
    "        // loop over rows\n",
    "        for (int r = 0, r < K, r++)\n",
    "        {\n",
    "                // loop over columns\n",
    "                // only in OpenCL 2.0\n",
    "                __attribute__ ((opencl unroll hint(2)))\n",
    "                for(c = 0, c < K, c++)\n",
    "                {\n",
    "                        sum += filt[r*K+c]*in[((y+r)*W+x)+c];\n",
    "                }\n",
    "        }\n",
    "        out[y*W+x] = sum + pBias;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Thread-Level Parallelism\n",
    "The final aspect of performance tuning is parallelization across cores. This is not direct or explicit under OpenCL, but can be configured by careful selection of **global** and **local** workgroup sizes. When they're equal, all work items are scheduled onto a single core. The ratio between these workgroup sizes indicates the number of threads/cores you can target. The OpenCL runtime can also be configured through **Device Fission** settings to partition the OpenCL device and restrict OpenCL operations onto a subset of available threads/cores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
