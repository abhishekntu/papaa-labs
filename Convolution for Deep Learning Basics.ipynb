{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Convolution\n",
    "\n",
    "Now, we have previously covered 2D Convolution earlier to explain the fundamental recurring operation in deep learning stacks for image processing/vision analytics today. Deep learning is \"deep\" because it organizes the flow of data into multiple layers ordered one after another. Typical deep learing routines are simple linear pipelines, but others can be arbitrary acyclic graphs. In all these cases, the use of convolution in any layer can be though of as a batch of multiple 2D convolutions -- or, 3D convolutions. We still operate on 2D images, and 2D kernels, but we compute of several of them in batched fashion. This is good as it represents another easy form of parallelism we can readily exploit. But is it also challenging at the same time when considering the storage requirements imposed by the third dimension for kernels as well as the resulting image maps.\n",
    "\n",
    "If we package up a 2D convolution in the **convolve2D** method we saw earlier, it is possible to represent the 3D convolution computation as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// loop over all the output maps of the convolution layer\n",
    "for(int out_map=0;out_map<M;out_map++) {\n",
    "    // loop over all the input maps (once for each output maps)\n",
    "    for(int in_map=0;in_map<N;in_map++) {\n",
    "        // convolve all input maps accumulate the result in output_maps[out_map]\n",
    "        // the 2D filter kernels are stored in kernel[][] structure.\n",
    "        convolve2D(input_maps[in_map],kernel[in_map][out_map],output_maps[out_map]);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the code above looks simple, it imposes a large operation complexity and associated memory bandwidth and storage cost. For efficient mapping, we must carefully reason about what loop to parallelize and how to parallelize it. We can visually represent this operation in the figure shown below.\n",
    "\n",
    "![](convolve3d.1.png) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
