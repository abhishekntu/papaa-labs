{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution exploiting GPUs\n",
    "\n",
    "GPUs offer a highly parallel substrate for accelerating data-parallel computations. Most modern GPU architectures are organized as clusters, which are analogous to **Compute Unit** in OpenCL. Each cluster contains several ALUs, called **Processing Elements** in OpenCL-speak, and a **Local Memory** that is available for shared communication within the cluster. Multiple threads of work, or **work items** can be mapped to a cluster depending on the workgroup scheduling. A GPU matches the OpenCL hardware model to the closest extent among existing OpenCL-compatible hardware platforms.\n",
    "\n",
    "1. Local/Constant Memory Optimizations\n",
    "Apart from SIMD, Threading, and Unrolling optimizations, the GPU substrate provides a unique optimization opportunity in the form of memory selection. Both **constant** and **local** memories can be used effectively to help accelerate your OpenCL task. It requires writing OpenCL kernels in a specific way to exploit these memories. This optimization is typically useless on a CPU as neither of these memories are explicitly available, and default to the processor cache. \n",
    "- To use **constant** memory, we simply need to tag the relevant data structures with the **__constant** identifier. The memory allocation on the host also needs to be tagged with **CL_MEM_READ_ONLY** qualifier to help copy the data from the host to the correct RAM on the OpenCL device.\n",
    "- To use **local** memory, we can only do so by declaring a fixed-size array within the OpenCL kernel body and declaring it with the qualifier **__local**. Also, we have to explicitly copy the data from the **__global** memory to the **__local** memory structures ourselves. Depending on the size of the workgroup and the **__local** structures, we can divvy up the memory loading task across multiple work-items. \n",
    "\n",
    "For 2D convolution, we can store the kernels in **__constant** memory, and prefetch portions of the input image into **__local** memory. Alternatively, when considering 3D convolution tasks (multiple 2D convolutions), we can store the output image in **__local** memory instead. We can visualize the memory hierarchy in OpenCL below.\n",
    "\n",
    "![](memory-hierarchy.png)\n",
    "\n",
    "We now show the OpenCL kernel for 2D convolution optimized for using **constant** and **local** memories below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__kernel void convolve(\n",
    "        const __global float *in,               // W*H input images\n",
    "        __constant float *filt,                 // K*K filter kernel\n",
    "        __global float *out,                    // W*H output images\n",
    "        const int K,                            // filter resolution\n",
    "        const float pBias)                      // constant offset/bias\n",
    "{\n",
    "        // get pixel position\n",
    "        const int W = get_global_size(0);\n",
    "        const int H = get_global_size(1);\n",
    "\n",
    "        // get image resolution\n",
    "        const int x = get_global_id(0); \n",
    "        const int y = get_global_id(1);\n",
    "\n",
    "        // allocate local RAM for storing input pixels\n",
    "        __local in_local[W*H];\n",
    "\n",
    "        // load data into the local RAM\n",
    "        in_local[x*W+y] = in[x*W+y];\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "        float sum = 0;\n",
    "        int c = 0;\n",
    "\n",
    "        // loop over rows\n",
    "        for (int r = 0, r < K, r++)\n",
    "        {\n",
    "                // loop over columns\n",
    "                for(c = 0, c < K, c++)\n",
    "                {\n",
    "                        sum += filt[r*K+c]*in_local[((y+r)*W+x)+c];\n",
    "                }\n",
    "        }\n",
    "        out[y*W+x] = sum + pBias;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code, we tag the **filt** structure as a **constant** and let OpenCL load the data into constant memory with the host code shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        d_filter = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, K*K, h_filter, &err);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other portion of code that has been added is the allocation, loading, and use of **in_local** variable. By preloading the image into the GPU local memory, accesses to the pixels are now fast. This is important as each pixel in convolution is read $K$x$K$ times. While caching can help reduce overheads of repetitive accesses, some accesses may miss the cache resulting in wasted cycles. We extract the newly added portion of code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        // allocate local RAM for storing input pixels\n",
    "        __local in_local[W*H];\n",
    "\n",
    "        // load data into the local RAM\n",
    "        in_local[x*W+y] = in[x*W+y];\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **barrier** call forces all workitems in the workgroup to synchornize before proceeding to the computation. This is important as all pixels must complete their memory loads before we can use them to do filtering. By enforcing (1) barrier, and (2) memory fence, both synchronization and completion of memory loads is guaranteed. Remember, this only operates on workitems inside a single workgroup on a single compute unit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
